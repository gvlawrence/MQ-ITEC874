{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Data Curation and NLP - Big Data Technology</h1><br /></center>\n",
    "<center>Tutorial 3 - Fred Amouzgar</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<centre><img src=\"https://github.com/FredAmouzgar/BigData/blob/master/pics/NLP_wordmap.jpg?raw=true\"></centre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLP Libraries in Python (data pre-processing, Tokenisation and Curation)</h3><br />\n",
    "1- NLTK: the most widely-mentioned NLP library<br />\n",
    "2- SpaCy:  an industrial-strength NLP library built for performance<br />\n",
    "3- Stanford CoreNLP: Written in Java with Python Wrapper, well-known for its speed<br />\n",
    "4- TextBlob: a user-friendly and intuitive NLTK interface<br />\n",
    "5- Polyglot: an open-source NLP library<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<centre><img src=\"https://github.com/FredAmouzgar/BigData/blob/master/pics/NLTK.png?raw=true\"></centre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK – Natural Language ToolKit</h3><br />\n",
    "The most popular Python NLP library\n",
    "It is a suite of libraries for statistical and symbolic Natural Language Processing\n",
    "NLTK is mostly an educational and research tool\n",
    "Good for Prototyping and Heavy for production\n",
    "Relevant Libraries:\n",
    "- Lexical Analysis: Word and Text Tokeniser\n",
    "- Part of Speech Tagger (POS tagging): Grammatical tagging and word-category\n",
    "- Name Entity Recognition (NER): Locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<centre><img src=\"https://github.com/FredAmouzgar/BigData/blob/master/pics/nlu_nlp.png?raw=true\" width=\"600px\" height=\"300px\"></centre>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK Installation</h3><br />\n",
    "- Find the Anaconda installation diresctory (Python and pip are usually under Scripts folder)<br />\n",
    "- Use pip/conda commands to install NLTK<br />\n",
    "- Then, download corpus and components as it is shown below<br />\n",
    "<centre><img src=\"https://github.com/FredAmouzgar/BigData/blob/master/pics/nltk_install.png?raw=true\"></centre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK Tokenization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a normal sentence.', 'This is the second sentence.']  has  2  elements.\n",
      "['That', \"'s\", 'a', 'good', 'dog', '.']  has  6  elements.\n",
      "['This', 'is', 'a', 'very', 'badly', 'formatted', 'sentence']  has  7  elements.\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize  import  TreebankWordTokenizer, RegexpTokenizer\n",
    "\n",
    "sentence1 = \"This is a normal sentence. This is the second sentence.\"\n",
    "word_list = word_tokenize(sentence1)  # Returns a list of words\n",
    "sents = sent_tokenize(sentence1) # Returns a list of sentences\n",
    "print(sents, \" has \", len(sents), \" elements.\")\n",
    "\n",
    "sentence2 = \"That's a good dog.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "sent2_words = tokenizer.tokenize(sentence2) # Returns a list like this: ['That', \"'s\", 'a', 'good', 'dog', '.’]\n",
    "print(sent2_words, \" has \", len(sent2_words), \" elements.\")\n",
    "\n",
    "sentence3 = \"This/is-a.very+badly)formatted,,sentence…\"\n",
    "regex_tokenizer = RegexpTokenizer(r'\\w+') # Dropping everything but alphanumerics\n",
    "sent3_words = regex_tokenizer.tokenize(sentence3) # Returns a list of words, discarding all the other chars.\n",
    "print(sent3_words, \" has \", len(sent3_words), \" elements.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK – Part-of-Speech (POS) Tagging</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fit', 'NNP')\n",
      "('my', 'PRP$')\n",
      "('galaxy', 'NN')\n",
      "('s2', 'NN')\n",
      "('perfectly', 'RB')\n",
      "('even', 'RB')\n",
      "('with', 'IN')\n",
      "('the', 'DT')\n",
      "('case', 'NN')\n",
      "('on', 'IN')\n",
      "('it', 'PRP')\n",
      "('.', '.')\n",
      "('Comfortable', 'JJ')\n",
      "('enough', 'RB')\n",
      "('to', 'TO')\n",
      "('wear', 'VB')\n",
      "('and', 'CC')\n",
      "('can', 'MD')\n",
      "('still', 'RB')\n",
      "('use', 'VB')\n",
      "('the', 'DT')\n",
      "('screen', 'NN')\n",
      "('through', 'IN')\n",
      "('the', 'DT')\n",
      "('plastic', 'NN')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'galaxy,s2,case,screen,plastic'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  nltk\n",
    "comment = \"Fit my galaxy s2 perfectly even with the case on it. Comfortable enough to wear and can still use the screen through the plastic\"\n",
    "text = nltk.word_tokenize(comment)\n",
    "p_tags = nltk.pos_tag(text)  # The MAIN tagger\n",
    "csv=\"\"\n",
    "for e in p_tags:\n",
    "    print(e)\n",
    "    if(e[1]=='NN'):\n",
    "        if(csv==\"\"):\n",
    "            csv=e[0]\n",
    "        else:\n",
    "            csv+=\",\"+e[0]\n",
    "csv  # Consists of a comma-separated string:\n",
    "# 'galaxy,s2,case,screen,plastic'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK POS tag meanings</h3><br />\n",
    "CC coordinating conjunction<br />\n",
    "CD cardinal digit<br />\n",
    "DT determiner<br />\n",
    "EX existential there (like: \"there is\" ... think of it like \"there exists\")<br />\n",
    "FW foreign word<br />\n",
    "IN preposition/subordinating conjunction<br />\n",
    "JJ adjective 'big'<br />\n",
    "JJR adjective, comparative 'bigger'<br />\n",
    "JJS adjective, superlative 'biggest'<br />\n",
    "LS list marker 1)<br />\n",
    "MD modal could, will<br />\n",
    "NN noun, singular 'desk'<br />\n",
    "NNS noun plural 'desks'<br />\n",
    "NNP proper noun, singular 'Harrison'<br />\n",
    "NNPS proper noun, plural 'Americans'<br />\n",
    "PDT predeterminer 'all the kids'<br />\n",
    "POS possessive ending parent's<br />\n",
    "PRP personal pronoun I, he, she<br />\n",
    "PRP\\$ possessive pronoun my, his, hers<br />\n",
    "RB adverb very, silently,<br />\n",
    "RBR adverb, comparative better<br />\n",
    "RBS adverb, superlative best<br />\n",
    "RP particle give up<br />\n",
    "TO to go 'to' the store.<br />\n",
    "UH interjection errrrrrrrm<br />\n",
    "VB verb, base form take<br />\n",
    "VBD verb, past tense took<br />\n",
    "VBG verb, gerund/present participle taking<br />\n",
    "VBN verb, past participle taken<br />\n",
    "VBP verb, sing. present, non-3d take<br />\n",
    "VBZ verb, 3rd person sing. present takes<br />\n",
    "WDT wh-determiner which<br />\n",
    "WP wh-pronoun who, what<br />\n",
    "WP\\$ possessive wh-pronoun whose<br />\n",
    "WRB wh-abverb where, when<br />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLTK – Name Entity Recognition</h3><br />\n",
    "- NER is the first step towards information extraction from unstructured text.\n",
    "- Extracting what is a real world entity from the text (Person, Organization, Event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Amin/NNP)\n",
      "  and/CC\n",
      "  (GPE Fred/NNP)\n",
      "  are/VBP\n",
      "  working/VBG\n",
      "  for/IN\n",
      "  (ORGANIZATION Macquarie/NNP)\n",
      "  university/NN\n",
      "  since/IN\n",
      "  2017/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"Amin and Fred are working for Macquarie university since 2017.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Practical</h2><br /><hr />\n",
    "<h3>Database:</h3><br />\n",
    "1- Download the Amazon JSON file (<a href=\"https://drive.google.com/file/d/1VKFoztinuLthitrSxPva8TcKPghcPR0B/view?usp=sharing\">Amazon.zip</a>) and extract it. It has 194,439 reviews for different cell phones.<br /><br />\n",
    "\n",
    "2- Import it into MongoDB, use “amazon” for your database name and “reviews” for you collection name. Please note that this file is <b>NOT</b> a JSON Array so the jsonArray option in $mongoimport$ command is not necessary.<br /><br />\n",
    "\n",
    "3- With the right query and from the Mongo command line count the number of imported reviews.<br /><br />\n",
    "\n",
    "<hr />\n",
    "<h3>Application:</h3><br /><br />\n",
    "1- Develop a Python application that connects to the amazon database and query all the reviews consist of “Nokia” in their “reviewText” field.<br /><br />\n",
    "\n",
    "2- Use NLTK to identify all the keywords in the “reviewText” field (verbs, Adjectives and Nouns).<br /><br />\n",
    "\n",
    "\n",
    "3- Put the keywords in a comma-separated string and add that to its relevant field in the database. Use the format below: \n",
    "“reviewKeywords”:”searching,battery,baby,monitor,Levana,BABYVIEW20,found,uses,Nokia,battery,was,nervous”<br /><br />\n",
    "\n",
    "4- (Optional) Use the Name Entity Recognition in NLTK to recognise name entities in the extracted keywords. Add another record called Entities consists of these objects.<br /><br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Testing:</h3>\n",
    "\n",
    "- Query and check that all the Nokia reviews have the newly-added record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
